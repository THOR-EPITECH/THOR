# Module Speech-to-Text

Module de reconnaissance vocale (speech-to-text) utilisant Whisper pour le projet THOR.

## Table des mati√®res

1. [Installation](#installation)
2. [Utilisation rapide](#utilisation-rapide)
3. [Architecture](#architecture)
4. [API](#api)
5. [Mod√®les disponibles](#mod√®les-disponibles)
6. [Exemples](#exemples)
7. [Int√©gration avec NLP](#int√©gration-avec-nlp)
8. [Tests](#tests)

---

## Installation

### D√©pendances

```bash
pip install -r requirements.txt
```

Les d√©pendances principales sont :
- `openai-whisper` : Mod√®le Whisper pour la transcription
- `torch` : Framework PyTorch
- `numpy` : Calculs num√©riques
- `sounddevice` : Capture audio depuis le microphone

---

## Utilisation rapide

### Transcription depuis le microphone

```python
from src.speech_to_text.factory import SpeechToTextFactory

# Cr√©er un mod√®le
model = SpeechToTextFactory.create("whisper-base")

# Transcrire depuis le microphone
result = model.transcribe_from_microphone(duration=5.0, language="fr")

if result.is_valid:
    print(f"Texte transcrit: {result.text}")
    print(f"Confiance: {result.confidence:.2%}")
```

### Transcription d'un fichier audio

```python
from pathlib import Path

# Transcrire un fichier
audio_path = Path("data/audio/commande.wav")
result = model.transcribe(audio_path, language="fr")

print(result.text)
```

---

## Architecture

Le module suit l'architecture d√©finie dans la documentation du projet :

```
src/speech_to_text/
‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îî‚îÄ‚îÄ model_interface.py      # Interface commune (SpeechToTextInterface)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ whisper_model.py        # Impl√©mentation Whisper
‚îú‚îÄ‚îÄ factory.py                   # Factory pour cr√©er les mod√®les
‚îî‚îÄ‚îÄ README.md                    # Cette documentation
```

### Principes

- **Interface commune** : Tous les mod√®les impl√©mentent `SpeechToTextInterface`
- **Factory Pattern** : Cr√©ation simple via `SpeechToTextFactory`
- **Extensibilit√©** : Facile d'ajouter d'autres mod√®les (Wav2Vec2, etc.)

---

## API

### SpeechToTextFactory

Factory pour cr√©er des instances de mod√®les.

#### M√©thodes

##### `create(model_type: str, **kwargs) -> SpeechToTextInterface`

Cr√©e une instance d'un mod√®le speech-to-text.

**Param√®tres :**
- `model_type` (str) : Type de mod√®le (`whisper-tiny`, `whisper-base`, etc.)
- `**kwargs` : Arguments additionnels (ex: `model_size`, `device`)

**Retourne :**
- Instance du mod√®le impl√©mentant `SpeechToTextInterface`

**Exemple :**
```python
model = SpeechToTextFactory.create("whisper-base")
model = SpeechToTextFactory.create("whisper-small", device="cuda")
```

##### `list_available_models() -> list[str]`

Liste tous les mod√®les disponibles.

**Retourne :**
- Liste des noms de mod√®les disponibles

**Exemple :**
```python
models = SpeechToTextFactory.list_available_models()
# ['whisper-tiny', 'whisper-base', 'whisper-small', ...]
```

### SpeechToTextInterface

Interface commune pour tous les mod√®les de reconnaissance vocale.

#### M√©thodes

##### `transcribe(audio_path: str | Path, language: Optional[str] = None) -> TranscriptionResult`

Transcrit un fichier audio en texte.

**Param√®tres :**
- `audio_path` : Chemin vers le fichier audio
- `language` : Code langue ISO (ex: `"fr"`, `"en"`). Si `None`, d√©tection automatique.

**Retourne :**
- `TranscriptionResult` : R√©sultat de la transcription

##### `transcribe_from_bytes(audio_bytes: bytes, sample_rate: int = 16000, language: Optional[str] = None) -> TranscriptionResult`

Transcrit des donn√©es audio brutes en texte.

**Param√®tres :**
- `audio_bytes` : Donn√©es audio brutes (format PCM 16-bit)
- `sample_rate` : Taux d'√©chantillonnage en Hz (d√©faut: 16000)
- `language` : Code langue ISO (optionnel)

**Retourne :**
- `TranscriptionResult` : R√©sultat de la transcription

##### `transcribe_from_microphone(duration: float = 5.0, sample_rate: int = 16000, language: Optional[str] = None) -> TranscriptionResult`

Transcrit l'audio captur√© depuis le microphone en temps r√©el.

**Param√®tres :**
- `duration` : Dur√©e d'enregistrement en secondes (d√©faut: 5.0)
- `sample_rate` : Taux d'√©chantillonnage en Hz (d√©faut: 16000)
- `language` : Code langue ISO (optionnel)

**Retourne :**
- `TranscriptionResult` : R√©sultat de la transcription

### TranscriptionResult

R√©sultat de transcription audio en texte.

**Attributs :**
- `text` (str) : Texte transcrit
- `language` (Optional[str]) : Langue d√©tect√©e (code ISO)
- `confidence` (float) : Score de confiance (0.0 √† 1.0)
- `segments` (Optional[list[dict]]) : Segments temporels si disponibles
- `is_valid` (bool) : True si la transcription est valide
- `error_message` (Optional[str]) : Message d'erreur si √©chec

**Exemple :**
```python
result = model.transcribe("audio.wav")

if result.is_valid:
    print(f"Texte: {result.text}")
    print(f"Langue: {result.language}")
    print(f"Confiance: {result.confidence:.2%}")
else:
    print(f"Erreur: {result.error_message}")
```

---

## Mod√®les disponibles

### Whisper

Mod√®le transformer open-source d√©velopp√© par OpenAI, sp√©cialis√© dans la transcription multilingue.

| Mod√®le | Vitesse | Pr√©cision | Taille | Usage recommand√© |
|--------|---------|-----------|--------|-------------------|
| `whisper-tiny` | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | 39 MB | Tests rapides |
| `whisper-base` | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 74 MB | **Usage g√©n√©ral (recommand√©)** |
| `whisper-small` | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 244 MB | Pr√©cision importante |
| `whisper-medium` | üêå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 769 MB | Pr√©cision maximale |
| `whisper-large` | üêåüêå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 1550 MB | Recherche/Production |

### Caract√©ristiques

- **Multilingue** : Supporte 99 langues dont le fran√ßais
- **Pr√©cision √©lev√©e** : Excellent pour le fran√ßais avec accents
- **Open-source** : Gratuit et libre d'utilisation
- **Optimis√©** : Param√®tres optimis√©s pour am√©liorer la pr√©cision

### Param√®tres optimis√©s

Le mod√®le Whisper utilise des param√®tres optimis√©s pour am√©liorer la pr√©cision :

- `temperature=0.0` : D√©terminisme maximal
- `beam_size=5` : Beam search pour explorer plusieurs hypoth√®ses
- `best_of=5` : S√©lection du meilleur r√©sultat parmi 5 tentatives
- `condition_on_previous_text=True` : Utilise le contexte pour la coh√©rence

### Preprocessing audio

Le module applique automatiquement un preprocessing pour am√©liorer la qualit√© :

- **Normalisation du volume** : Peak normalization √† 0.95
- **R√©duction du bruit** : Filtre passe-bas l√©ger
- **Suppression du silence** : Retrait automatique du silence au d√©but/fin

---

## Exemples

### Exemple 1 : Transcription simple

```python
from src.speech_to_text.factory import SpeechToTextFactory
from pathlib import Path

# Cr√©er le mod√®le
model = SpeechToTextFactory.create("whisper-base")

# Transcrire un fichier
result = model.transcribe(Path("audio.wav"), language="fr")

print(f"Texte: {result.text}")
```

### Exemple 2 : Transcription depuis le microphone

```python
from src.speech_to_text.factory import SpeechToTextFactory

model = SpeechToTextFactory.create("whisper-base")

# Enregistrer et transcrire
result = model.transcribe_from_microphone(
    duration=5.0,
    language="fr"
)

if result.is_valid:
    print(f"Vous avez dit: {result.text}")
```

### Exemple 3 : Comparaison de mod√®les

```python
from src.speech_to_text.factory import SpeechToTextFactory

audio_path = "commande.wav"

# Tester diff√©rents mod√®les
for model_type in ["whisper-tiny", "whisper-base", "whisper-small"]:
    model = SpeechToTextFactory.create(model_type)
    result = model.transcribe(audio_path, language="fr")
    
    print(f"{model_type}: {result.text}")
    print(f"  Confiance: {result.confidence:.2%}")
```

### Exemple 4 : Gestion des erreurs

```python
from src.speech_to_text.factory import SpeechToTextFactory

model = SpeechToTextFactory.create("whisper-base")
result = model.transcribe("fichier_inexistant.wav")

if not result.is_valid:
    print(f"Erreur: {result.error_message}")
    # Erreur: Fichier audio non trouv√©: fichier_inexistant.wav
```

---

## Int√©gration avec NLP

Le texte transcrit peut √™tre directement pass√© au module NLP pour extraire les villes :

```python
from src.speech_to_text.factory import SpeechToTextFactory
from src.nlp.factory import NLPModelFactory

# 1. Transcrire l'audio
stt_model = SpeechToTextFactory.create("whisper-base")
audio_result = stt_model.transcribe_from_microphone(
    duration=5.0,
    language="fr"
)

if not audio_result.is_valid:
    print(f"Erreur de transcription: {audio_result.error_message}")
    exit(1)

# 2. Extraire les villes depuis le texte transcrit
nlp_model = NLPModelFactory.create("camembert")
nlp_result = nlp_model.extract(audio_result.text)

if nlp_result.is_valid:
    print(f"D√©part: {nlp_result.departure}")
    print(f"Destination: {nlp_result.destination}")
else:
    print(f"Erreur d'extraction: {nlp_result.error_message}")
```

### Pipeline complet

```python
# Pipeline complet : Audio -> Texte -> Extraction villes
def process_voice_command(audio_path: str):
    # √âtape 1: Speech-to-Text
    stt = SpeechToTextFactory.create("whisper-base")
    transcription = stt.transcribe(audio_path, language="fr")
    
    if not transcription.is_valid:
        return None
    
    # √âtape 2: NLP
    nlp = NLPModelFactory.create("camembert")
    extraction = nlp.extract(transcription.text)
    
    return extraction
```

---

## Tests

### Tests unitaires

Voir `tests/speech_to_text/test_speech_to_text.py` pour les tests complets.

### Ex√©cution des tests

```bash
# Test basique
python tests/speech_to_text/test_speech_to_text.py --basic

# Test avec microphone
python tests/speech_to_text/test_speech_to_text.py --mic

# Test avec fichier audio
python tests/speech_to_text/test_speech_to_text.py --file audio.wav

# Test en boucle
python tests/speech_to_text/test_speech_to_text.py --mic-loop

# Voir toutes les options
python tests/speech_to_text/test_speech_to_text.py --help
```

### Options de test

- `--basic` : Test basique (v√©rification du module)
- `--models` : Test de cr√©ation des mod√®les
- `--list` : Liste les mod√®les disponibles
- `--mic` : Test avec le microphone
- `--mic-loop` : Test microphone en boucle
- `--file <chemin>` : Test avec un fichier audio
- `--duration <sec>` : Dur√©e d'enregistrement
- `--language <lang>` : Langue (fr/en)
- `--model <size>` : Taille du mod√®le (tiny/base/small)

---

## Conseils d'utilisation

### Pour am√©liorer la pr√©cision

1. **Utilisez un mod√®le plus grand** : `whisper-small` ou `whisper-medium`
2. **Parlez clairement** : Articulez bien les mots
3. **Environnement calme** : R√©duisez le bruit de fond
4. **Microphone de qualit√©** : Utilisez un bon microphone
5. **Dur√©e appropri√©e** : 3-10 secondes est optimal
6. **Sp√©cifiez la langue** : Toujours utiliser `language="fr"` pour le fran√ßais

### Pour am√©liorer la vitesse

1. **Utilisez un mod√®le plus petit** : `whisper-tiny` ou `whisper-base`
2. **R√©duisez la dur√©e** : Enregistrements plus courts
3. **Utilisez un GPU** : Si disponible, Whisper sera beaucoup plus rapide

### Formats audio support√©s

- `.wav` (recommand√©)
- `.mp3`
- `.m4a`
- `.flac`
- `.ogg`

### Langues support√©es

Whisper supporte 99 langues, notamment :
- Fran√ßais (`fr`)
- Anglais (`en`)
- Espagnol (`es`)
- Allemand (`de`)
- Italien (`it`)
- Et bien d'autres...

---

## R√©solution de probl√®mes

### Erreur : "sounddevice n'est pas install√©"

```bash
pip install sounddevice
```

### Erreur : "ModuleNotFoundError: No module named 'whisper'"

```bash
pip install -r requirements.txt
```

### Erreur SSL lors du t√©l√©chargement du mod√®le

Le module g√®re automatiquement les probl√®mes SSL. Si le probl√®me persiste :

1. V√©rifiez votre connexion internet
2. Si vous √™tes derri√®re un proxy, configurez-le :
   ```bash
   export https_proxy=http://proxy:port
   ```

### Transcription peu pr√©cise

- Utilisez un mod√®le plus grand (`whisper-small` ou `whisper-medium`)
- V√©rifiez la qualit√© audio (volume, bruit)
- Parlez plus clairement et plus lentement
- Sp√©cifiez explicitement `language="fr"`

### Transcription trop lente

- Utilisez un mod√®le plus petit (`whisper-tiny` ou `whisper-base`)
- R√©duisez la dur√©e d'enregistrement
- Utilisez un GPU si disponible

---

## Architecture technique

### Flux de traitement

```
Audio Input
    ‚Üì
Preprocessing (normalisation, r√©duction bruit)
    ‚Üì
Whisper Model (transcription)
    ‚Üì
Post-processing (extraction m√©tadonn√©es)
    ‚Üì
TranscriptionResult
```

### Interface commune

Tous les mod√®les impl√©mentent `SpeechToTextInterface` pour permettre :
- Le remplacement facile d'un mod√®le par un autre
- La comparaison √©quitable entre mod√®les
- Le benchmarking automatis√©
- L'ajout de nouveaux mod√®les sans modifier le code existant

### Extensibilit√©

Pour ajouter un nouveau mod√®le (ex: Wav2Vec2) :

1. Cr√©er une classe qui h√©rite de `SpeechToTextInterface`
2. Impl√©menter toutes les m√©thodes abstraites
3. Ajouter le mod√®le dans `SpeechToTextFactory._models`

Voir la documentation du projet pour plus de d√©tails sur l'architecture.

---

## R√©f√©rences

- [Documentation Whisper](https://github.com/openai/whisper)
- [Documentation du projet THOR](../docs/DOCUMENTATION.md)
- [Tests](../tests/README.md)

