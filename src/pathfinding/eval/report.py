"""
G√©n√©ration de rapports markdown pour les √©valuations Pathfinding.
"""
import json
from pathlib import Path
from typing import Dict, Any
from datetime import datetime
from src.common.io import read_jsonl


def save_report(
    output_dir: str | Path,
    model_name: str,
    dataset_path: str | Path,
    metrics: Dict[str, float]
) -> Path:
    """
    G√©n√®re et sauvegarde un rapport markdown.
    
    Args:
        output_dir: Dossier contenant les r√©sultats
        model_name: Nom du mod√®le √©valu√©
        dataset_path: Chemin vers le dataset utilis√©
        metrics: Dictionnaire des m√©triques
    
    Returns:
        Chemin du fichier de rapport cr√©√©
    """
    output_dir = Path(output_dir)
    report_path = output_dir / "report.md"
    
    # Charge les pr√©dictions
    predictions_path = output_dir / "predictions.jsonl"
    predictions = list(read_jsonl(predictions_path)) if predictions_path.exists() else []
    
    total_samples = len(predictions)
    route_found_count = sum(1 for p in predictions if p.get("route_found", False))
    
    # G√©n√®re le rapport
    report = f"""# Rapport d'√©valuation Pathfinding - {model_name}

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Mod√®le**: {model_name}  
**Dataset**: {dataset_path}  
**Nombre d'√©chantillons**: {total_samples}

---

## üìä M√©triques globales

### Pr√©cision Origine/Destination
- **Pr√©cision origine**: {metrics.get('origin_accuracy_mean', 0):.4f} ¬± {metrics.get('origin_accuracy_std', 0):.4f}
- **Pr√©cision destination**: {metrics.get('destination_accuracy_mean', 0):.4f} ¬± {metrics.get('destination_accuracy_std', 0):.4f}

### Taux de succ√®s
- **Itin√©raires trouv√©s**: {route_found_count} / {total_samples} ({route_found_count/total_samples*100:.1f}%)
- **Taux de succ√®s moyen**: {metrics.get('route_found_rate', 0):.4f}

### Pr√©cision du chemin
- **Pr√©cision du chemin moyenne**: {metrics.get('path_accuracy_mean', 'N/A')}
- **√âcart-type**: {metrics.get('path_accuracy_std', 'N/A')}

### Distance
- **Erreur de distance moyenne**: {metrics.get('distance_error_mean', 'N/A')} km
- **Erreur relative moyenne**: {metrics.get('distance_relative_error_mean', 'N/A')}

### Nombre d'√©tapes
- **Nombre d'√©tapes moyen**: {metrics.get('num_steps_mean', 0):.2f} ¬± {metrics.get('num_steps_std', 0):.2f}

---

## üìà Statistiques

- **Total d'√©chantillons**: {total_samples}
- **Itin√©raires trouv√©s**: {route_found_count} ({route_found_count/total_samples*100:.1f}%)
- **Itin√©raires non trouv√©s**: {total_samples - route_found_count} ({(total_samples - route_found_count)/total_samples*100:.1f}%)

---

## ‚úÖ Exemples d'itin√©raires trouv√©s

"""
    
    # Affiche quelques exemples r√©ussis
    successful = [p for p in predictions if p.get("route_found", False)][:5]
    for i, pred in enumerate(successful, 1):
        report += f"""### {i}. {pred.get('origin', 'N/A')} ‚Üí {pred.get('destination', 'N/A')}

- **√âtapes**: {len(pred.get('predicted_steps', []))}
- **Distance**: {pred.get('predicted_distance', 'N/A')} km
- **Pr√©cision du chemin**: {pred.get('path_accuracy', 'N/A')}

"""
    
    report += """---

## ‚ùå Exemples d'itin√©raires non trouv√©s

"""
    
    # Affiche quelques exemples √©chou√©s
    failed = [p for p in predictions if not p.get("route_found", False)][:5]
    for i, pred in enumerate(failed, 1):
        error = pred.get('error', 'Raison inconnue')
        report += f"""### {i}. {pred.get('origin', 'N/A')} ‚Üí {pred.get('destination', 'N/A')}

- **Erreur**: {error}

"""
    
    report += f"""
---

## üìÅ Fichiers

- **Dataset**: `{dataset_path}`
- **Pr√©dictions**: `predictions.jsonl`
- **M√©triques**: `metrics.json`
- **Rapport**: `report.md`

---

## üìù Notes

Ce rapport a √©t√© g√©n√©r√© automatiquement par le syst√®me d'√©valuation THOR.
"""
    
    # Sauvegarde
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    return report_path
